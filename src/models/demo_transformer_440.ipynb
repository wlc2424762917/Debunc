{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T04:25:49.202077Z",
     "start_time": "2024-12-10T04:25:49.192633Z"
    }
   },
   "source": [
    "import torch\n",
    "from models.common import RangeWeight\n",
    "# from models.modeling_mistral import MistralForCausalLM\n",
    "# from models.modeling_llama_442 import LlamaForCausalLM\n",
    "from models.modeling_llama import LlamaForCausalLM\n",
    "\n",
    "from transformers import AutoTokenizer"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T04:26:26.760555Z",
     "start_time": "2024-12-10T04:25:49.525512Z"
    }
   },
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_name = \"/data/hf_models/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "print(model.config)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanglichao/miniconda3/envs/debunc/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2078d9a979348578cae68f46420b498"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T05:33:13.407186Z",
     "start_time": "2024-12-10T05:33:13.401563Z"
    }
   },
   "source": [
    "prompt = \"\"\"Answer the following question based on the documents provided.\n",
    "Document 1: Today, the weather is raining.\n",
    "Document 2: Today, the weather is sunny.\n",
    "Question: What is the weather like today?\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "chat_prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "inputs = tokenizer(chat_prompt, add_special_tokens=False, return_tensors=\"pt\").to(\n",
    "    model.device\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T05:33:27.058297Z",
     "start_time": "2024-12-10T05:33:27.035070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(prompt))\n",
    "print(len(inputs[\"input_ids\"][0]))\n",
    "print(inputs[\"input_ids\"][0])\n",
    "print(\n",
    "        tokenizer.decode(inputs[\"input_ids\"][0])\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188\n",
      "51\n",
      "tensor([128000, 128006,    882, 128007,    271,  16533,    279,   2768,   3488,\n",
      "          3196,    389,    279,   9477,   3984,    627,   7676,    220,     16,\n",
      "            25,  11450,     11,    279,   9282,    374,  84353,    627,   7676,\n",
      "           220,     17,     25,  11450,     11,    279,   9282,    374,  40798,\n",
      "           627,  14924,     25,   3639,    374,    279,   9282,   1093,   3432,\n",
      "            30, 128009, 128006,  78191, 128007,    271], device='cuda:0')\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Answer the following question based on the documents provided.\n",
      "Document 1: Today, the weather is raining.\n",
      "Document 2: Today, the weather is sunny.\n",
      "Question: What is the weather like today?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T04:26:50.365908Z",
     "start_time": "2024-12-10T04:26:26.788563Z"
    }
   },
   "source": [
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=140,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "response = outputs[0][inputs[\"input_ids\"].shape[-1] : -1]\n",
    "print(\"Model output WITHOUT attention scaling:\")\n",
    "print(tokenizer.decode(response))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanglichao/miniconda3/envs/debunc/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/wanglichao/miniconda3/envs/debunc/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output WITHOUT attention scaling:\n",
      "A classic example of a conflicting information problem!\n",
      "\n",
      "Since we have two documents with different information about the weather, we can't determine the correct answer based on these documents alone. We need more information or a way to resolve the conflict.\n",
      "\n",
      "In this case, I would say that the answer is \"Unknown\" or \"Inconclusive\" because we have contradictory information.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T04:26:50.400025Z",
     "start_time": "2024-12-10T04:26:50.378219Z"
    }
   },
   "source": [
    "range_weights = [\n",
    "    RangeWeight(15, 26, 1),\n",
    "    RangeWeight(26, 37, 0),\n",
    "    # RangeWeight(15, 28, 1),\n",
    "    # RangeWeight(28, 41, 0),\n",
    "]\n",
    "\n",
    "for range_weight in range_weights:\n",
    "    print(f\"Giving the following text a weight of {range_weight.weight}:\")\n",
    "    print(\n",
    "        tokenizer.decode(inputs[\"input_ids\"][0][range_weight.start : range_weight.end])\n",
    "    )\n",
    "    print(\"---\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document\n",
      "Giving the following text a weight of 1:\n",
      "Document 1: Today, the weather is raining.\n",
      "\n",
      "---\n",
      "Giving the following text a weight of 0:\n",
      "Document 2: Today, the weather is sunny.\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T04:23:07.159999Z",
     "start_time": "2024-12-10T04:22:51.867894Z"
    }
   },
   "source": [
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=120,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    range_weights=range_weights,\n",
    ")\n",
    "response = outputs[0][:-1]\n",
    "print(\"Model output WITH attention scaling:\")\n",
    "print(tokenizer.decode(response))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output WITH attention scaling:\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Answer the following question based on the documents provided.\n",
      "Document 1: Today, the weather is raining.\n",
      "Document 2: Today, the weather is sunny.\n",
      "Question: What is the weather like today?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "A simple question!\n",
      "\n",
      "Since there are only two documents, and one of them says \"Today, the weather is raining\", I would conclude that the weather is indeed raining today.\n",
      "\n",
      "So, the answer is: The weather is raining.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### New Test Case"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T05:27:27.479996Z",
     "start_time": "2024-12-10T05:27:27.462394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"\n",
    "Document 1: Today, the weather is raining.\n",
    "Document 2: Today, the weather is sunny.\n",
    "Repeat the above content\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "chat_prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "inputs = tokenizer(chat_prompt, add_special_tokens=False, return_tensors=\"pt\").to(\n",
    "    model.device\n",
    ")\n",
    "print(len(prompt))\n",
    "print(len(inputs[\"input_ids\"][0]))\n",
    "print(inputs[\"input_ids\"][0])\n",
    "print(\n",
    "        tokenizer.decode(inputs[\"input_ids\"][0][3])\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n",
      "36\n",
      "tensor([128000, 128006,    882, 128007,    271,   7676,    220,     16,     25,\n",
      "         11450,     11,    279,   9282,    374,  84353,    627,   7676,    220,\n",
      "            17,     25,  11450,     11,    279,   9282,    374,  40798,    627,\n",
      "         39818,    279,   3485,   2262, 128009, 128006,  78191, 128007,    271],\n",
      "       device='cuda:0')\n",
      "<|end_header_id|>\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T05:30:16.423341Z",
     "start_time": "2024-12-10T05:30:16.417920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "range_weights = [\n",
    "    # RangeWeight(5, 16, 1),\n",
    "    # RangeWeight(16, 27, 0),\n",
    "     RangeWeight(5, 17, 1),\n",
    "    RangeWeight(17, 27, 0),\n",
    "]\n",
    "\n",
    "for range_weight in range_weights:\n",
    "    print(f\"Giving the following text a weight of {range_weight.weight}:\")\n",
    "    print(\n",
    "        tokenizer.decode(inputs[\"input_ids\"][0][range_weight.start : range_weight.end])\n",
    "    )\n",
    "    print(\"---\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Giving the following text a weight of 1:\n",
      "Document 1: Today, the weather is raining.\n",
      "Document\n",
      "---\n",
      "Giving the following text a weight of 0:\n",
      " 2: Today, the weather is sunny.\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T05:30:26.585834Z",
     "start_time": "2024-12-10T05:30:16.943098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=120,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    range_weights=range_weights,\n",
    ")\n",
    "response = outputs[0][:-1]\n",
    "print(\"Model output WITH attention scaling:\")\n",
    "print(tokenizer.decode(response))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output WITH attention scaling:\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Document 1: Today, the weather is raining.\n",
      "Document 2: Today, the weather is sunny.\n",
      "Repeat the above content<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Here is the repeated content:\n",
      "\n",
      "Document 1: Today, the weather is raining.\n",
      "Document 1: Today, the weather is raining.\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T05:27:51.359844Z",
     "start_time": "2024-12-10T05:27:41.987893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=120,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    range_weights=None,\n",
    ")\n",
    "response = outputs[0][:-1]\n",
    "print(\"Model output WITH attention scaling:\")\n",
    "print(tokenizer.decode(response))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output WITH attention scaling:\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Document 1: Today, the weather is raining.\n",
      "Document 2: Today, the weather is sunny.\n",
      "Repeat the above content<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Here are the repeated documents:\n",
      "\n",
      "Document 1: Today, the weather is raining.\n",
      "\n",
      "Document 2: Today, the weather is sunny.\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "debunc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
