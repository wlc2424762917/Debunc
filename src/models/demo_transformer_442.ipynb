{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T05:50:29.930048Z",
     "start_time": "2024-12-10T05:50:29.919825Z"
    }
   },
   "source": [
    "import torch\n",
    "from models.common import RangeWeight\n",
    "# from models.modeling_mistral import MistralForCausalLM\n",
    "from models.modeling_llama_442 import LlamaForCausalLM\n",
    "# from models.modeling_llama import LlamaForCausalLM\n",
    "\n",
    "from transformers import AutoTokenizer"
   ],
   "outputs": [],
   "execution_count": 91
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T05:58:20.913125Z",
     "start_time": "2024-12-10T05:58:14.985964Z"
    }
   },
   "source": [
    "# model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_name = \"/data/hf_models/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "print(model.config)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85ca83dfb01641388927c08faee70409"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"/data/hf_models/Llama-3.1-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T05:58:20.937456Z",
     "start_time": "2024-12-10T05:58:20.915199Z"
    }
   },
   "source": [
    "prompt = \"\"\"Answer the following question based on the documents provided.\n",
    "Document 1: Today, the weather is raining.\n",
    "Document 2: Today, the weather is sunny.\n",
    "Question: What is the weather like today?\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "chat_prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "inputs = tokenizer(chat_prompt, add_special_tokens=False, return_tensors=\"pt\").to(\n",
    "    model.device\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T05:59:10.370035Z",
     "start_time": "2024-12-10T05:59:10.362688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(prompt))\n",
    "print(len(inputs[\"input_ids\"][0]))\n",
    "print(inputs[\"input_ids\"][0])\n",
    "print(\n",
    "        tokenizer.decode(inputs[\"input_ids\"][0])\n",
    "    )\n",
    "print(\n",
    "        tokenizer.decode(inputs[\"input_ids\"][0][0:25])\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188\n",
      "76\n",
      "tensor([128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "            25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "           220,   1627,  10263,    220,   2366,     19,    271, 128009, 128006,\n",
      "           882, 128007,    271,  16533,    279,   2768,   3488,   3196,    389,\n",
      "           279,   9477,   3984,    627,   7676,    220,     16,     25,  11450,\n",
      "            11,    279,   9282,    374,  84353,    627,   7676,    220,     17,\n",
      "            25,  11450,     11,    279,   9282,    374,  40798,    627,  14924,\n",
      "            25,   3639,    374,    279,   9282,   1093,   3432,     30, 128009,\n",
      "        128006,  78191, 128007,    271], device='cuda:0')\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Answer the following question based on the documents provided.\n",
      "Document 1: Today, the weather is raining.\n",
      "Document 2: Today, the weather is sunny.\n",
      "Question: What is the weather like today?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T05:35:44.201044Z",
     "start_time": "2024-12-10T05:35:36.894967Z"
    }
   },
   "source": [
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=140,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "response = outputs[0][inputs[\"input_ids\"].shape[-1] : -1]\n",
    "print(\"Model output WITHOUT attention scaling:\")\n",
    "print(tokenizer.decode(response))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanglichao/miniconda3/envs/debunc/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/wanglichao/miniconda3/envs/debunc/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "range_weights in llamaModel None\n",
      "Model output WITHOUT attention scaling:\n",
      "Based on the provided documents, there are two different answers to the question. \n",
      "\n",
      "Document 1 states that the weather is raining, while Document 2 states that the weather is sunny. \n",
      "\n",
      "To determine the correct answer, we would need more information or a more specific document that indicates which one is accurate. However, based on the information given, we can say that there are two conflicting answers:\n",
      "\n",
      "- According to Document 1, the weather is raining.\n",
      "- According to Document 2, the weather is sunny.\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T05:51:07.161090Z",
     "start_time": "2024-12-10T05:51:07.150119Z"
    }
   },
   "source": [
    "range_weights = [\n",
    "    # RangeWeight(15+25, 26+25, 1),\n",
    "    # RangeWeight(26+25, 37+25, 0),\n",
    "    RangeWeight(15, 26, 1),\n",
    "    RangeWeight(26, 37, 0),\n",
    "]\n",
    "for range_weight in range_weights:\n",
    "    print(f\"Giving the following text a weight of {range_weight.weight}:\")\n",
    "    print(\n",
    "        tokenizer.decode(inputs[\"input_ids\"][0][range_weight.start : range_weight.end])\n",
    "    )\n",
    "    print(\"---\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Giving the following text a weight of 1:\n",
      "Document 1: Today, the weather is raining.\n",
      "\n",
      "---\n",
      "Giving the following text a weight of 0:\n",
      "Document 2: Today, the weather is sunny.\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "execution_count": 95
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T05:48:01.201158Z",
     "start_time": "2024-12-10T05:47:54.489964Z"
    }
   },
   "source": [
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=100,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    range_weights=range_weights,\n",
    ")\n",
    "response = outputs[0][inputs[\"input_ids\"].shape[-1] :-1]\n",
    "print(\"Model output WITH attention scaling:\")\n",
    "print(tokenizer.decode(response))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=15, end=23, weight=1), RangeWeight(start=23, end=37, weight=0)]\n",
      "Model output WITH attention scaling:\n",
      "A simple question!\n",
      "\n",
      "Since there is only one document, and it's a general statement about the weather, I would say that the weather is not specified in the document. Therefore, I cannot determine what the weather is like today based on the provided documents.\n",
      "\n",
      "In other words, Document 1 does not provide any information about the current weather conditions.\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### New Test Case"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T05:44:43.606665Z",
     "start_time": "2024-12-10T05:44:43.596232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"\n",
    "Document 1: Today, the weather is raining.\n",
    "Document 2: Today, the weather is sunny.\n",
    "Repeat the above content\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "chat_prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "inputs = tokenizer(chat_prompt, add_special_tokens=False, return_tensors=\"pt\").to(\n",
    "    model.device\n",
    ")\n",
    "print(len(prompt))\n",
    "print(len(inputs[\"input_ids\"][0]))\n",
    "print(inputs[\"input_ids\"][0])\n",
    "print(\n",
    "        tokenizer.decode(inputs[\"input_ids\"][0])\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n",
      "36\n",
      "tensor([128000, 128006,    882, 128007,    271,   7676,    220,     16,     25,\n",
      "         11450,     11,    279,   9282,    374,  84353,    627,   7676,    220,\n",
      "            17,     25,  11450,     11,    279,   9282,    374,  40798,    627,\n",
      "         39818,    279,   3485,   2262, 128009, 128006,  78191, 128007,    271],\n",
      "       device='cuda:0')\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Document 1: Today, the weather is raining.\n",
      "Document 2: Today, the weather is sunny.\n",
      "Repeat the above content<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T05:44:45.747736Z",
     "start_time": "2024-12-10T05:44:45.742606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "range_weights = [\n",
    "    RangeWeight(5, 16, 1),\n",
    "    RangeWeight(16, 27, 0),\n",
    "    #  RangeWeight(5+25, 16+25, 1),\n",
    "    # RangeWeight(16+25, 27+25, 0),\n",
    "]\n",
    "\n",
    "for range_weight in range_weights:\n",
    "    print(f\"Giving the following text a weight of {range_weight.weight}:\")\n",
    "    print(\n",
    "        tokenizer.decode(inputs[\"input_ids\"][0][range_weight.start : range_weight.end])\n",
    "    )\n",
    "    print(\"---\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Giving the following text a weight of 1:\n",
      "Document 1: Today, the weather is raining.\n",
      "\n",
      "---\n",
      "Giving the following text a weight of 0:\n",
      "Document 2: Today, the weather is sunny.\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T05:44:50.186086Z",
     "start_time": "2024-12-10T05:44:48.721580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=120,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    range_weights=range_weights,\n",
    ")\n",
    "response = outputs[0][:-1]\n",
    "print(\"Model output WITH attention scaling:\")\n",
    "print(tokenizer.decode(response))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range_weights in llamaModel [RangeWeight(start=5, end=16, weight=1), RangeWeight(start=16, end=27, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=5, end=16, weight=1), RangeWeight(start=16, end=27, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=5, end=16, weight=1), RangeWeight(start=16, end=27, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=5, end=16, weight=1), RangeWeight(start=16, end=27, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=5, end=16, weight=1), RangeWeight(start=16, end=27, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=5, end=16, weight=1), RangeWeight(start=16, end=27, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=5, end=16, weight=1), RangeWeight(start=16, end=27, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=5, end=16, weight=1), RangeWeight(start=16, end=27, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=5, end=16, weight=1), RangeWeight(start=16, end=27, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=5, end=16, weight=1), RangeWeight(start=16, end=27, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=5, end=16, weight=1), RangeWeight(start=16, end=27, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=5, end=16, weight=1), RangeWeight(start=16, end=27, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=5, end=16, weight=1), RangeWeight(start=16, end=27, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=5, end=16, weight=1), RangeWeight(start=16, end=27, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=5, end=16, weight=1), RangeWeight(start=16, end=27, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=5, end=16, weight=1), RangeWeight(start=16, end=27, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=5, end=16, weight=1), RangeWeight(start=16, end=27, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=5, end=16, weight=1), RangeWeight(start=16, end=27, weight=0)]\n",
      "range_weights in llamaModel [RangeWeight(start=5, end=16, weight=1), RangeWeight(start=16, end=27, weight=0)]\n",
      "Model output WITH attention scaling:\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Document 1: Today, the weather is raining.\n",
      "Document 2: Today, the weather is sunny.\n",
      "Repeat the above content<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Here are the documents:\n",
      "\n",
      "**Document 1:**\n",
      "Today, the weather is raining.\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "debunc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
